{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Daily Rainfall Prediction - Global (All 32 Cities)\n",
                "\n",
                "This notebook trains a Transformer model to predict daily rainfall using data from all 32 cities worldwide.\n",
                "\n",
                "**Model:** Past 30 days \u2192 Next day's rainfall\n",
                "\n",
                "**Cities:** All 32 cities in the dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, random_split, Dataset\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Dataset Classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DailyWeatherDataset(Dataset):\n",
                "    \"\"\"\u8bfb\u53d6\u65e5\u7ea7\u5929\u6c14\u6570\u636e\uff0c\u53ef\u4ee5\u7b5b\u9009\u7279\u5b9a\u57ce\u5e02\u6216\u4f7f\u7528\u6240\u6709\u57ce\u5e02\"\"\"\n",
                "\n",
                "    def __init__(self, csv_path=\"./dataset_global/weather_daily_global.csv\", cities=None, date_col=\"time\"):\n",
                "        if not os.path.exists(csv_path):\n",
                "            raise FileNotFoundError(f\"\u274c \u627e\u4e0d\u5230\u65e5\u7ea7\u6570\u636e\u6587\u4ef6: {csv_path}\")\n",
                "\n",
                "        df = pd.read_csv(csv_path)\n",
                "\n",
                "        # \u7b5b\u9009\u57ce\u5e02\uff08\u5982\u679c cities=None\uff0c\u5219\u4f7f\u7528\u6240\u6709\u57ce\u5e02\uff09\n",
                "        if cities is not None:\n",
                "            df = df[df[\"city\"].isin(cities)].copy()\n",
                "        else:\n",
                "            df = df.copy()\n",
                "\n",
                "        df[\"date\"] = pd.to_datetime(df[date_col])\n",
                "        df = df.sort_values([\"city\", \"date\"]).reset_index(drop=True)\n",
                "\n",
                "        target_col = \"precipitation_sum_mm\"\n",
                "        feature_cols = [\"temp_mean_C\", \"temp_max_C\", \"temp_min_C\", \"rh_mean_pct\",\n",
                "                       \"press_mean_hPa\", \"wind_mean_ms\", \"wind_dir_deg\", \"cloud_mean_pct\",\n",
                "                       \"dew_point_C\", \"month\", \"month_sin\", \"month_cos\",\n",
                "                       \"precip_lag_1\", \"precip_lag_3\", \"temp_mean_lag_1\", \"precip_roll7\",\n",
                "                       \"lat\", \"lon\"]\n",
                "\n",
                "        # \u6e05\u6d17 NaN/Inf\n",
                "        cols_to_check = feature_cols + [target_col]\n",
                "        df[cols_to_check] = df[cols_to_check].replace([np.inf, -np.inf], np.nan)\n",
                "        before = len(df)\n",
                "        df = df.dropna(subset=cols_to_check).reset_index(drop=True)\n",
                "        print(f\"\ud83e\uddf9 \u5df2\u53bb\u9664\u542b NaN/Inf \u7684\u884c: {before - len(df)} \u884c\uff0c\u5269\u4f59 {len(df)} \u884c\")\n",
                "\n",
                "        self.feature_cols = feature_cols\n",
                "        self.target_col = target_col\n",
                "\n",
                "        # \u6807\u51c6\u5316\n",
                "        self.mean = df[feature_cols].mean()\n",
                "        self.std = df[feature_cols].std().replace(0, 1e-6)\n",
                "        df[feature_cols] = (df[feature_cols] - self.mean) / (self.std + 1e-6)\n",
                "        self.df = df.reset_index(drop=True)\n",
                "\n",
                "        print(f\"\u2705 \u57ce\u5e02: {sorted(self.df['city'].unique())}\")\n",
                "        print(f\"\u2705 \u603b\u5929\u6570: {len(self.df)}, \u7279\u5f81\u7ef4\u5ea6: {len(self.feature_cols)}\")\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        x = torch.tensor(row[self.feature_cols].astype(float).values, dtype=torch.float32)\n",
                "        y = torch.tensor(float(row[self.target_col]), dtype=torch.float32)\n",
                "        return x, y, row[\"city\"], row[\"date\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DailySequenceDataset(Dataset):\n",
                "    \"\"\"\u628a DailyWeatherDataset \u8f6c\u6210\u5e8f\u5217\uff1a\u8fc7\u53bb lookback \u5929 \u2192 \u660e\u5929 (log1p)\"\"\"\n",
                "\n",
                "    def __init__(self, daily_ds, lookback=30):\n",
                "        self.daily_ds = daily_ds\n",
                "        self.feature_cols = daily_ds.feature_cols\n",
                "        self.target_col = daily_ds.target_col\n",
                "        self.lookback = lookback\n",
                "        self.df = daily_ds.df\n",
                "        self.samples = []\n",
                "\n",
                "        for city, df_city in self.df.groupby(\"city\"):\n",
                "            idxs = df_city.index.to_list()\n",
                "            if len(idxs) <= lookback:\n",
                "                continue\n",
                "            for i in range(lookback, len(idxs)):\n",
                "                self.samples.append((city, idxs[i - lookback], idxs[i]))\n",
                "\n",
                "        print(f\"\u2705 \u5e8f\u5217\u6837\u672c\u6570: {len(self.samples)}, lookback = {lookback} \u5929\")\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        city, start, end = self.samples[idx]\n",
                "        window = self.df.loc[start:end-1, self.feature_cols].values.astype(\"float32\")\n",
                "        target = float(self.df.loc[end, self.target_col])\n",
                "        x = torch.tensor(window, dtype=torch.float32)\n",
                "        y_log = torch.tensor(np.log1p(target), dtype=torch.float32)\n",
                "        return x, y_log"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Device Selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_device():\n",
                "    if torch.cuda.is_available():\n",
                "        device = torch.device(\"cuda\")\n",
                "    elif torch.backends.mps.is_available():\n",
                "        device = torch.device(\"mps\")\n",
                "    else:\n",
                "        device = torch.device(\"cpu\")\n",
                "    print(f\"\ud83e\udde0 \u4f7f\u7528\u8bbe\u5907: {device}\")\n",
                "    return device\n",
                "\n",
                "device = get_device()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RainfallTransformer(nn.Module):\n",
                "    def __init__(self, input_dim, seq_len=30, d_model=128, nhead=4, num_layers=3, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.input_dim = input_dim\n",
                "        self.seq_len = seq_len\n",
                "        self.input_proj = nn.Linear(input_dim, d_model)\n",
                "        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, d_model))\n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=d_model, nhead=nhead, dim_feedforward=4*d_model,\n",
                "            dropout=dropout, activation=\"gelu\", batch_first=True)\n",
                "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
                "        self.head = nn.Sequential(\n",
                "            nn.LayerNorm(d_model), nn.Linear(d_model, 128), nn.GELU(),\n",
                "            nn.Dropout(dropout), nn.Linear(128, 1))\n",
                "\n",
                "    def forward(self, x):\n",
                "        if x.size(1) > self.seq_len:\n",
                "            x = x[:, -self.seq_len:, :]\n",
                "        x_proj = self.input_proj(x) + self.pos_embedding[:, :x.size(1)]\n",
                "        enc = self.encoder(x_proj)\n",
                "        pooled = enc.mean(dim=1)\n",
                "        return self.head(pooled).squeeze(-1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
                "    model.train()\n",
                "    total_loss = 0.0\n",
                "    for xb, yb in tqdm(loader, desc=\"\ud83d\udc5f \u8bad\u7ec3\"):\n",
                "        xb, yb = xb.to(device), yb.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        pred = model(xb)\n",
                "        loss = criterion(pred, yb)\n",
                "        loss.backward()\n",
                "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        optimizer.step()\n",
                "        total_loss += loss.item() * xb.size(0)\n",
                "    return total_loss / len(loader.dataset)\n",
                "\n",
                "def evaluate(model, loader, criterion, device):\n",
                "    model.eval()\n",
                "    total_loss = 0.0\n",
                "    all_pred_log, all_true_log = [], []\n",
                "    with torch.no_grad():\n",
                "        for xb, yb in tqdm(loader, desc=\"\ud83d\udd0e \u9a8c\u8bc1\"):\n",
                "            xb, yb = xb.to(device), yb.to(device)\n",
                "            pred = model(xb)\n",
                "            loss = criterion(pred, yb)\n",
                "            total_loss += loss.item() * xb.size(0)\n",
                "            all_pred_log.append(pred.cpu())\n",
                "            all_true_log.append(yb.cpu())\n",
                "    avg_loss = total_loss / len(loader.dataset)\n",
                "    pred_log = torch.cat(all_pred_log).numpy()\n",
                "    true_log = torch.cat(all_true_log).numpy()\n",
                "    rmse_log = float(np.sqrt(np.mean((pred_log - true_log) ** 2)))\n",
                "    pred_mm = np.expm1(pred_log)\n",
                "    true_mm = np.expm1(true_log)\n",
                "    rmse_mm = float(np.sqrt(np.mean((pred_mm - true_mm) ** 2)))\n",
                "    mae_mm = float(np.mean(np.abs(pred_mm - true_mm)))\n",
                "    return avg_loss, rmse_log, rmse_mm, mae_mm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Load Data - ALL Cities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "daily_ds = DailyWeatherDataset(\n",
                "    csv_path=\"./dataset_global/weather_daily_global.csv\",\n",
                "    cities=None,  # None = all cities\n",
                "    date_col=\"time\")\n",
                "\n",
                "print(f\"\\n\ud83d\udcca \u57ce\u5e02\u6570\u91cf: {daily_ds.df['city'].nunique()}\")\n",
                "print(f\"\u603b\u6837\u672c\u6570: {len(daily_ds)}\")\n",
                "\n",
                "seq_ds = DailySequenceDataset(daily_ds, lookback=30)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6.5. Print Normalization Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print normalization statistics for use in production\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"ðŸ“Š NORMALIZATION STATISTICS (Copy these to weather_service.py)\")\n",
                "print(\"=\"*80)\n",
                "print(\"\\nFeature columns:\")\n",
                "for i, col in enumerate(daily_ds.feature_cols):\n",
                "    print(f\"  {i}: {col}\")\n",
                "\n",
                "print(\"\\nDAILY_FEATURE_MEAN = np.array([\")\n",
                "for i, (col, val) in enumerate(zip(daily_ds.feature_cols, daily_ds.mean)):\n",
                "    print(f\"    {val:.6f},  # {col}\")\n",
                "print(\"])\")\n",
                "\n",
                "print(\"\\nDAILY_FEATURE_STD = np.array([\")\n",
                "for i, (col, val) in enumerate(zip(daily_ds.feature_cols, daily_ds.std)):\n",
                "    print(f\"    {val:.6f},  # {col}\")\n",
                "print(\"])\")\n",
                "print(\"\\n\" + \"=\"*80 + \"\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train/Val Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "val_size = int(len(seq_ds) * 0.2)\n",
                "train_size = len(seq_ds) - val_size\n",
                "train_ds, val_ds = random_split(seq_ds, [train_size, val_size],\n",
                "                                generator=torch.Generator().manual_seed(42))\n",
                "print(f\"\u8bad\u7ec3\u96c6: {train_size}, \u9a8c\u8bc1\u96c6: {val_size}\")\n",
                "\n",
                "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
                "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Initialize Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = RainfallTransformer(input_dim=len(daily_ds.feature_cols), seq_len=30).to(device)\n",
                "criterion = nn.HuberLoss(delta=0.1)\n",
                "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
                "\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"\ud83e\udd16 \u603b\u53c2\u6570: {total_params:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_epochs = 15\n",
                "best_val_loss = float(\"inf\")\n",
                "best_path = \"daily_transformer_global_best.pt\"\n",
                "train_losses, val_losses = [], []\n",
                "\n",
                "for epoch in range(1, num_epochs + 1):\n",
                "    print(f\"\\n===== Epoch {epoch}/{num_epochs} =====\")\n",
                "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
                "    print(f\"\ud83d\udcc9 \u8bad\u7ec3\u635f\u5931: {train_loss:.6f}\")\n",
                "    val_loss, rmse_log, rmse_mm, mae_mm = evaluate(model, val_loader, criterion, device)\n",
                "    print(f\"\u2705 \u9a8c\u8bc1\u635f\u5931: {val_loss:.6f}, RMSE(mm): {rmse_mm:.4f}, MAE(mm): {mae_mm:.4f}\")\n",
                "    train_losses.append(train_loss)\n",
                "    val_losses.append(val_loss)\n",
                "    if val_loss < best_val_loss:\n",
                "        best_val_loss = val_loss\n",
                "        torch.save(model.state_dict(), best_path)\n",
                "        print(f\"\ud83d\udcbe \u5df2\u4fdd\u5b58\u6700\u4f18\u6a21\u578b: {best_path}\")\n",
                "\n",
                "print(f\"\\n\ud83c\udf89 \u8bad\u7ec3\u5b8c\u6210\uff01\u6700\u4f73\u9a8c\u8bc1\u635f\u5931: {best_val_loss:.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Plot Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Train\", marker=\"o\")\n",
                "plt.plot(range(1, len(val_losses)+1), val_losses, label=\"Val\", marker=\"s\")\n",
                "plt.xlabel(\"Epoch\")\n",
                "plt.ylabel(\"Loss\")\n",
                "plt.title(\"Training & Validation Loss (All 32 Cities)\")\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.savefig(\"loss_curve_global.png\", dpi=150)\n",
                "plt.show()\n",
                "print(\"\ud83d\udcc8 \u5df2\u4fdd\u5b58 loss_curve_global.png\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}